## 第4章作业

#### 4.13

*proof*：
$$
假设i和j属于一个等价类,即i\leftrightarrow j，且i是常返的\\
(1) 当\pi_i>0时，即i是正常返的\\
设m和n使P_{ij}^mP_{ji}^n>0,且假设P_{ii}^s>0\\
\therefore P_{jj}^{m+n+s}\geq P_{ji}^nP_{ii}^sP_{ij}^m>0\\
\lim_{n\rightarrow s}P_{jj}^{m+n+s}\geq P_{ji}^nP_{ii}^s\lim_{n\rightarrow s}P_{ii}^s\\
\because \lim_{n\rightarrow s}P_{ii}^s = \pi_i\\
\pi_j\geq \pi>0\\
\therefore j是正常返的\\
(2)当\pi=0，即i是零常返的\\
同理，可设m和n使P_{ij}^mP_{ji}^n>0,且假设P_{jj}^s>0\\
\therefore 推出 0=\pi_i\geq \pi_j>0\\
\therefore j是零常返的\\
综上，正（零）常返的是等价类的一个性质。
$$

#### 4.16

*(a)*

取在她当前所在地点的雨伞数为状态，转移概率是
$$
P_{0,r}=1,P_{i,r-i}=1-p,P_{i,r-i+1}=p,\quad i=1,\dots,r
$$
*(b)*

极限概率的方程是
$$
\pi_r=\pi_0+\pi_1p,\\
\pi_j=\pi_{r-j}(1-p)+P_{r-i+1}p,\quad j=1,\dots,r-1\\
$$
易验证他们满足
$$
\pi_i=
\begin{cases}
\frac{q}{r+q} \quad 若i=0\\
\frac{1}{r+q}\quad 若i=1,\dots,r
\end{cases}
$$
其中$$q=1-p.$$

*(c)*
$$
p\pi_0=\frac{pq}{r+q}
$$

#### 4.21

其正常返的当且仅当，如下方程组有解：
$$
y_0=y_1q_1,y_j=y_{j+1}q_{j+1}+y_{j-1}q_{j-1}, j\geq 1\\
$$
具有满足$$y_j\geq0,\sum_{j}y_j=1$$的一个解
$$
\therefore y_{j+1}q_{j+1}=y_jp_j,j\geq 0\\
\therefore y_{j+1}=y_0\frac{p_0\dots p_j}{q_1\dots q_{j+1}},j\geq0\\
$$
其充分必要条件为:
$$
\sum_{j=0}^\infty \frac{p_0\dots p_j}{q_1\dots q_{j+1}}<\infty.
$$

#### 4.30

$$
由于P\{X_i-Y_i=1\}=P_1\{1-P_2\}和P\{X_i-Y_i=-1\}=P_2\{1-P_1\},\\
又由简单随机徘徊得：\\
P=\frac{P_1(1-P_2)}{P_1(1-P_2)+P_2(1-P_!)}\\
类比赌徒破产问题，得：\\
\begin{align}
P\{误差\}
&=P\{在到达M前下降M\}\\
&=1-\frac{1-(1-(q/p)^M)}{1-(q/p)^{2M}}\\
&=\frac{(q/p)^M}{1+(q/p)^M}\\
&=\frac{1}{(q/p)^M+1}\\
&=\frac{1}{1+\lambda^M}\\
\end{align}
\\\therefore 根据Wald方程，有\\
E[\sum_{i=1}^N(X_i-Y_i)]=E[N](P_1-P_2)=\frac{M(\lambda^M-1)}{1+\lambda^M}\\
$$

#### 4.43

对$$1,2,\dots,n$$的任意排列$$i_1,i_2,\dots,i_n$$,以$\pi(i_1,i_2,\dots,i_n)$记在前一位规则下的极限概率。根据时间可逆性，对一切排列有
$$
(*)P_{i_{j+1}}\pi(i_1,i_2,\dots,i_n)=P_{i_{j}}\pi(i_1,i_2,\dots,i_n).
$$
现在需求的元素的平均位置可以表之为
$$
\begin{align}
平均位置
&=\sum_{i}P_iE[元素i的位置]\\
&=\sum_iP_i[1+\sum_{i\ne j}P\{元素j在i前\}]\\
&=1+\sum_i\sum_{i\ne j}P_iP\{e_j在e_i前\}\\
&=1+\sum\sum_{i<j}(P_i-P_j)P\{e_j在e_i前\}+\sum\sum_{i<j}P_j
\end{align}
$$
因此，为了使需求的元素位置最小，要使$$P\{e_j在e_i前\}在P_j>P_i$$时尽量最大，而在$P_i>P_j$时尽量最小。现在，在移至最前面地规则下
$$
P\{e_j在e_i前\}>\frac{P_j}{P_j+P_i}\\
$$
现在考虑$i在j$前地任意状态，比如$(\dots i,i_1,\dots,i_k,j,\dots).$利用（*）连续的转移位置，我们有
$$
\pi(\dots i,i_1,\dots,i_k,j,\dots)=(\frac{P_i}{P_j})^{k+1}\pi(\dots j,i_1,\dots,i_k,i,\dots)
$$
现在当$P_j>P_i$时上式蕴含
$$
\pi(\dots i,i_1,\dots,i_k,j,\dots)<(\frac{P_i}{P_j})\pi(\dots j,i_1,\dots,i_k,i,\dots).
$$
令$\alpha(i,j)=P\{e_i在e_j前\}$，由对$i在j$前的状态求和，且利用上面的事实，我们可见
$$
\alpha(i,j)<\frac{P_i}{P_j}\alpha(j,i)\\
$$
由于$\alpha(i,j)=1-\alpha(j,i)$，它导致
$$
\alpha(j,i)>\frac{P_j}{P_j+P_i}
$$
